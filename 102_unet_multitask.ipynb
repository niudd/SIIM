{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset_unet import prepare_trainset\n",
    "from utils import save_checkpoint, load_checkpoint, set_logger\n",
    "from gpu_utils import set_n_get_device\n",
    "\n",
    "from model.model_unet_multitask import UNetResNet34, predict_proba\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====MODEL ACHITECTURE: UNetResNet34====\n"
     ]
    }
   ],
   "source": [
    "######### Config the training process #########\n",
    "#device = set_n_get_device(\"0, 1, 2, 3\", data_device_id=\"cuda:0\")#0, 1, 2, 3, IMPORTANT: data_device_id is set to free gpu for storing the model, e.g.\"cuda:1\"\n",
    "MODEL = 'UNetResNet34'#'RESNET34', 'RESNET18', 'INCEPTION_V3', 'BNINCEPTION', 'SEResnet50'\n",
    "#AUX_LOGITS = True#False, only for 'INCEPTION_V3'\n",
    "print('====MODEL ACHITECTURE: %s===='%MODEL)\n",
    "\n",
    "device = set_n_get_device(\"0\", data_device_id=\"cuda:0\")#0, 1, 2, 3, IMPORTANT: data_device_id is set to free gpu for storing the model, e.g.\"cuda:1\"\n",
    "multi_gpu = None#[0, 1]#use 2 gpus\n",
    "\n",
    "SEED = 1234#5678#4567#3456#2345#1234\n",
    "debug = True# if True, load 100 samples\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 24\n",
    "warm_start, last_checkpoint_path = False, 'checkpoint/%s_%s_v1_seed%s/best.pth.tar'%(MODEL, IMG_SIZE, SEED)\n",
    "checkpoint_path = 'checkpoint/multitask_%s_%s_v1_seed%s'%(MODEL, IMG_SIZE, SEED)\n",
    "LOG_PATH = 'logging/multitask_%s_%s_v1_seed%s.log'%(MODEL, IMG_SIZE, SEED)#\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "early_stopping_round = 10#500#50\n",
    "LearningRate = 0.02#phase1: 0.02, phase2: 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of trainset (for training):  900\n",
      "Count of validset (for training):  100\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = prepare_trainset(BATCH_SIZE, NUM_WORKERS, SEED, IMG_SIZE, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, (images, masks) in enumerate(train_dl):\n",
    "    images = images.to(device=device, dtype=torch.float)\n",
    "    masks = masks.to(device=device, dtype=torch.float)\n",
    "    if i==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 512, 512]), torch.Size([4, 1, 512, 512]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size(), masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define non-empty-mask as 1\n",
    "truth_clf = (masks.sum(dim=2).sum(dim=2)!=0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_clf.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNetResNet34(debug=True).cuda(device=device)\n",
    "\n",
    "#torch.cuda.set_device(0)\n",
    "#torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')\n",
    "#net = DistributedDataParallel(net, device_ids=[0], output_device=0)\n",
    "#torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "if multi_gpu is not None:\n",
    "    net = nn.DataParallel(net, device_ids=multi_gpu)\n",
    "\n",
    "#checkpoint_path = 'checkpoint/UNetResNet34_256_v1_seed1234/best.pth.tar'\n",
    "#net, _ = load_checkpoint(checkpoint_path, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  torch.Size([4, 3, 512, 512])\n",
      "e1 torch.Size([4, 64, 256, 256])\n",
      "e2 torch.Size([4, 64, 256, 256])\n",
      "e3 torch.Size([4, 128, 128, 128])\n",
      "e4 torch.Size([4, 256, 64, 64])\n",
      "e5 torch.Size([4, 512, 32, 32])\n",
      "center torch.Size([4, 256, 16, 16])\n",
      "d5 torch.Size([4, 64, 32, 32])\n",
      "d4 torch.Size([4, 64, 64, 64])\n",
      "d3 torch.Size([4, 64, 128, 128])\n",
      "d2 torch.Size([4, 64, 256, 256])\n",
      "d1 torch.Size([4, 64, 512, 512])\n",
      "hypercolum torch.Size([4, 320, 512, 512])\n",
      "logit_mask torch.Size([4, 1, 512, 512])\n",
      "avgpool torch.Size([4, 256, 1, 1])\n",
      "reshape torch.Size([4, 256])\n",
      "logit_clf torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/data/endi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/bigdata/data/endi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "logit_mask, logit_clf = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(0.1681, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(0.6941, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, loss_mask, loss_clf = net.criterion(logit_mask, masks, logit_clf, truth_clf)\n",
    "loss, loss_mask, loss_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(logit_mask).view(4, -1) * (torch.sigmoid(logit_clf).view(4, -1)>0.75).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0375, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_metric = net.metric(logit_mask, masks)\n",
    "_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# i = 1\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 5))\n",
    "# if masks[i].mean()==0:\n",
    "#     plt.title('Empty mask')\n",
    "# else:\n",
    "#     plt.title('See marker')\n",
    "\n",
    "# ax = fig.add_subplot(1, 2, 1)\n",
    "# plt.imshow(image.cpu().numpy()[i][0], cmap=plt.cm.bone)\n",
    "# plt.imshow(masks.cpu().numpy()[i][0], alpha=0.3, cmap=\"Reds\")\n",
    "\n",
    "# ax = fig.add_subplot(1, 2, 2)\n",
    "# plt.imshow(image.cpu().numpy()[i][0], cmap=plt.cm.bone)\n",
    "# plt.imshow((logit>0).float().cpu().detach().numpy()[i][0], alpha=0.3, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import save_checkpoint, load_checkpoint, set_logger\n",
    "from gpu_utils import set_n_get_device\n",
    "\n",
    "from dataset_unet import prepare_trainset\n",
    "from model.model_unet_multitask import UNetResNet34, predict_proba\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Define the training process #########\n",
    "def run_check_net(train_dl, val_dl, multi_gpu=[0, 1]):\n",
    "    set_logger(LOG_PATH)\n",
    "    logging.info('\\n\\n')\n",
    "    #---\n",
    "    if MODEL == 'UNetResNet34':\n",
    "        net = UNetResNet34(debug=False).cuda(device=device)\n",
    "    #elif MODEL == 'RESNET18':\n",
    "    #    net = AtlasResNet18(debug=False).cuda(device=device)\n",
    "\n",
    "#     for param in net.named_parameters():\n",
    "#         if param[0][:8] in ['decoder5']:#'decoder5', 'decoder4', 'decoder3', 'decoder2'\n",
    "#             param[1].requires_grad = False\n",
    "\n",
    "    # dummy sgd to see if it can converge ...\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                      lr=LearningRate, momentum=0.9, weight_decay=0.0001)\n",
    "    #optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.045)#LearningRate\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
    "                                                           factor=0.5, patience=4,#4 resnet34 \n",
    "                                                           verbose=False, threshold=0.0001, \n",
    "                                                           threshold_mode='rel', cooldown=0, \n",
    "                                                           min_lr=0, eps=1e-08)\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.9, last_epoch=-1)\n",
    "    \n",
    "    if warm_start:\n",
    "        logging.info('warm_start: '+last_checkpoint_path)\n",
    "        net, _ = load_checkpoint(last_checkpoint_path, net)\n",
    "    \n",
    "    # using multi GPU\n",
    "    if multi_gpu is not None:\n",
    "        net = nn.DataParallel(net, device_ids=multi_gpu)\n",
    "\n",
    "    diff = 0\n",
    "    best_val_metric = -0.1\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #seed = get_seed()\n",
    "    #seed = SEED\n",
    "    #logging.info('aug seed: '+str(seed))\n",
    "    #ia.imgaug.seed(seed)\n",
    "    #np.random.seed(seed)\n",
    "    \n",
    "    for i_epoch in range(NUM_EPOCHS):\n",
    "        t0 = time.time()\n",
    "        # iterate through trainset\n",
    "        if multi_gpu is not None:\n",
    "            net.module.set_mode('train')\n",
    "        else:\n",
    "            net.set_mode('train')\n",
    "        train_loss_list = []\n",
    "        train_loss_mask_list = []\n",
    "        train_loss_clf_list = []\n",
    "        train_metric_list = []\n",
    "        #for seed in [1]:#[1, SEED]:#augment raw data with a duplicate one (augmented)\n",
    "        #seed = get_seed()\n",
    "        #np.random.seed(seed)\n",
    "        #ia.imgaug.seed(i//10)\n",
    "        for i, (image, masks) in enumerate(train_dl):\n",
    "            input_data = image.to(device=device, dtype=torch.float)\n",
    "            truth_mask = masks.to(device=device, dtype=torch.float)\n",
    "            #define non-empty-mask as 1\n",
    "            truth_clf = (truth_mask.sum(dim=2).sum(dim=2)!=0).float()\n",
    "            #set_trace()\n",
    "            logit_mask, logit_clf = net(input_data)#[:, :3, :, :]\n",
    "            \n",
    "            if multi_gpu is not None:\n",
    "                _train_loss, _train_loss_mask, _train_loss_clf = net.module.criterion(logit_mask, truth_mask, logit_clf, truth_clf)\n",
    "                _train_metric = net.module.metric(logit_mask, truth_mask)#device='gpu'\n",
    "            else:\n",
    "                _train_loss, _train_loss_mask, _train_loss_clf = net.criterion(logit_mask, truth_mask, logit_clf, truth_clf)\n",
    "                _train_metric  = net.metric(logit_mask, truth_mask)#device='gpu'\n",
    "            train_loss_list.append(_train_loss.item())\n",
    "            train_loss_mask_list.append(_train_loss_mask.item())\n",
    "            train_loss_clf_list.append(_train_loss_clf.item())\n",
    "            train_metric_list.append(_train_metric.item())#.detach()\n",
    "\n",
    "            #grandient accumulation step=2\n",
    "            acc_step = 2\n",
    "            _train_loss = _train_loss / acc_step\n",
    "            _train_loss.backward()\n",
    "            if (i+1)%acc_step==0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        train_loss = np.mean(train_loss_list)\n",
    "        train_loss_mask = np.mean(train_loss_mask_list)\n",
    "        train_loss_clf = np.mean(train_loss_clf_list)\n",
    "        train_metric = np.mean(train_metric_list)\n",
    "\n",
    "        # compute valid loss & metrics (concatenate valid set in cpu, then compute loss, metrics on full valid set)\n",
    "        net.module.set_mode('valid')\n",
    "        with torch.no_grad():\n",
    "#             val_loss_list, val_metric_list = [], []\n",
    "#             for i, (image, masks) in enumerate(val_dl):\n",
    "#                 input_data = image.to(device=device, dtype=torch.float)\n",
    "#                 truth = masks.to(device=device, dtype=torch.float)\n",
    "#                 logit = net(input_data)\n",
    "                \n",
    "#                 if multi_gpu is not None:\n",
    "#                     _val_loss  = net.module.criterion(logit, truth)\n",
    "#                     _val_metric  = net.module.metric(logit, truth)#device='gpu'\n",
    "#                 else:\n",
    "#                     _val_loss  = net.criterion(logit, truth)\n",
    "#                     _val_metric  = net.metric(logit, truth)#device='gpu'\n",
    "#                 val_loss_list.append(_val_loss.item())\n",
    "#                 val_metric_list.append(_val_metric.item())#.detach()\n",
    "\n",
    "#             val_loss = np.mean(val_loss_list)\n",
    "#             val_metric = np.mean(val_metric_list)\n",
    "            \n",
    "            logit_mask_valid, logit_clf_valid, truth_mask_valid, truth_clf_valid = None, None, None, None\n",
    "            for j, (image, masks) in enumerate(val_dl):\n",
    "                input_data = image.to(device=device, dtype=torch.float)\n",
    "                truth_mask = masks.cpu().float()\n",
    "                #define non-empty-mask as 1\n",
    "                truth_clf = (truth_mask.sum(dim=2).sum(dim=2)!=0).cpu().float()\n",
    "                logit_mask, logit_clf = net(input_data)\n",
    "                logit_mask, logit_clf = logit_mask.cpu().float(), logit_clf.cpu().float()\n",
    "                if logit_mask_valid is None:\n",
    "                    logit_mask_valid = logit_mask\n",
    "                    logit_clf_valid = logit_clf\n",
    "                    truth_mask_valid = truth_mask\n",
    "                    truth_clf_valid = truth_clf\n",
    "                else:\n",
    "                    logit_mask_valid = torch.cat((logit_mask_valid, logit_mask), dim=0)\n",
    "                    logit_clf_valid = torch.cat((logit_clf_valid, logit_clf), dim=0)\n",
    "                    truth_mask_valid = torch.cat((truth_mask_valid, truth_mask), dim=0)\n",
    "                    truth_clf_valid = torch.cat((truth_clf_valid, truth_clf), dim=0)\n",
    "            if multi_gpu is not None:\n",
    "                val_loss, val_loss_mask, val_loss_clf = net.module.criterion(logit_mask_valid, truth_mask_valid, logit_clf_valid, truth_clf_valid)\n",
    "                val_metric = net.module.metric(logit_mask_valid, truth_mask_valid)\n",
    "            else:\n",
    "                val_loss, val_loss_mask, val_loss_clf = net.criterion(logit_mask_valid, truth_mask_valid, logit_clf_valid, truth_clf_valid)\n",
    "                val_metric = net.metric(logit_mask_valid, truth_mask_valid)\n",
    "\n",
    "        # Adjust learning_rate\n",
    "        scheduler.step(val_metric)\n",
    "        #\n",
    "        if val_metric > best_val_metric:\n",
    "            best_val_metric = val_metric\n",
    "            is_best = True\n",
    "            diff = 0\n",
    "        else:\n",
    "            is_best = False\n",
    "            diff += 1\n",
    "            if diff > early_stopping_round:\n",
    "                logging.info('Early Stopping: val_metric does not increase %d rounds'%early_stopping_round)\n",
    "                #print('Early Stopping: val_iou does not increase %d rounds'%early_stopping_round)\n",
    "                break\n",
    "        \n",
    "        #save checkpoint\n",
    "        checkpoint_dict = \\\n",
    "        {\n",
    "            'epoch': i,\n",
    "            'state_dict': net.module.state_dict() if multi_gpu is not None else net.state_dict(),\n",
    "            'optim_dict' : optimizer.state_dict(),\n",
    "            'metrics': {'train_loss': train_loss, 'val_loss': val_loss, \n",
    "                        'train_metric': train_metric, 'val_metric': val_metric}\n",
    "        }\n",
    "        save_checkpoint(checkpoint_dict, is_best=is_best, checkpoint=checkpoint_path)\n",
    "\n",
    "        #if i_epoch%20==0:\n",
    "        if i_epoch>-1:\n",
    "            logging.info('[EPOCH %05d]train_loss_mask, train_loss_clf, train_metric: %0.5f, %0.5f, %0.5f; val_loss_mask, val_loss_clf, val_metric: %0.5f, %0.5f, %0.5f; time elapsed: %0.1f min'%(i_epoch, train_loss_mask.item(), train_loss_clf.item(), train_metric.item(), val_loss_mask.item(), val_loss_clf.item(), val_metric.item(), (time.time()-t0)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====MODEL ACHITECTURE: UNetResNet34====\n"
     ]
    }
   ],
   "source": [
    "######### Config the training process #########\n",
    "#device = set_n_get_device(\"0, 1, 2, 3\", data_device_id=\"cuda:0\")#0, 1, 2, 3, IMPORTANT: data_device_id is set to free gpu for storing the model, e.g.\"cuda:1\"\n",
    "MODEL = 'UNetResNet34'#'RESNET34', 'RESNET18', 'INCEPTION_V3', 'BNINCEPTION', 'SEResnet50'\n",
    "#AUX_LOGITS = True#False, only for 'INCEPTION_V3'\n",
    "print('====MODEL ACHITECTURE: %s===='%MODEL)\n",
    "\n",
    "device = set_n_get_device(\"1, 2\", data_device_id=\"cuda:0\")#0, 1, 2, 3, IMPORTANT: data_device_id is set to free gpu for storing the model, e.g.\"cuda:1\"\n",
    "multi_gpu = [0, 1]#use 2 gpus\n",
    "\n",
    "SEED = 1234#5678#4567#3456#2345#1234\n",
    "debug = True# if True, load 100 samples\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 24\n",
    "warm_start, last_checkpoint_path = False, 'checkpoint/%s_%s_v1_seed%s/best.pth.tar'%(MODEL, IMG_SIZE, SEED)\n",
    "checkpoint_path = 'checkpoint/multitask_%s_%s_v1_seed%s'%(MODEL, IMG_SIZE, SEED)\n",
    "LOG_PATH = 'logging/multitask_%s_%s_v1_seed%s.log'%(MODEL, IMG_SIZE, SEED)#\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "early_stopping_round = 10#500#50\n",
    "LearningRate = 0.02#phase1: 0.02, phase2: 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of trainset (for training):  900\n",
      "Count of validset (for training):  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "/home/bigdata/data/endi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/bigdata/data/endi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "[EPOCH 00000]train_loss_mask, train_loss_clf, train_metric: 0.07749, 0.66904, 0.70538; val_loss_mask, val_loss_clf, val_metric: 0.02297, 0.56849, 0.77083; time elapsed: 1.3 min\n",
      "[EPOCH 00001]train_loss_mask, train_loss_clf, train_metric: 0.01710, 0.53081, 0.79167; val_loss_mask, val_loss_clf, val_metric: 0.01845, 0.53771, 0.77083; time elapsed: 1.2 min\n",
      "[EPOCH 00002]train_loss_mask, train_loss_clf, train_metric: 0.01595, 0.51189, 0.79167; val_loss_mask, val_loss_clf, val_metric: 0.02012, 0.52884, 0.77083; time elapsed: 1.2 min\n",
      "[EPOCH 00003]train_loss_mask, train_loss_clf, train_metric: 0.01562, 0.50201, 0.79167; val_loss_mask, val_loss_clf, val_metric: 0.01434, 0.52588, 0.77083; time elapsed: 1.2 min\n",
      "[EPOCH 00004]train_loss_mask, train_loss_clf, train_metric: 0.01476, 0.49792, 0.79167; val_loss_mask, val_loss_clf, val_metric: 0.01485, 0.51645, 0.77083; time elapsed: 1.2 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ec7bf772c8b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m######### Run the training process #########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrun_check_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-2506c2835fd0>\u001b[0m in \u001b[0;36mrun_check_net\u001b[0;34m(train_dl, val_dl, multi_gpu)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#ia.imgaug.seed(i//10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtruth_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m#define empty-mask as 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######### Load data #########\n",
    "train_dl, val_dl = prepare_trainset(BATCH_SIZE, NUM_WORKERS, SEED, IMG_SIZE, debug)\n",
    "\n",
    "######### Run the training process #########\n",
    "run_check_net(train_dl, val_dl, multi_gpu=multi_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict the validset, and analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import save_checkpoint, load_checkpoint, set_logger\n",
    "from gpu_utils import set_n_get_device\n",
    "\n",
    "from model.model_unet_multitask import UNetResNet34, predict_proba\n",
    "from dataset_unet import prepare_trainset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====MODEL ACHITECTURE: UNetResNet34====\n"
     ]
    }
   ],
   "source": [
    "######### Config the training process #########\n",
    "#device = set_n_get_device(\"0, 1, 2, 3\", data_device_id=\"cuda:0\")#0, 1, 2, 3, IMPORTANT: data_device_id is set to free gpu for storing the model, e.g.\"cuda:1\"\n",
    "MODEL = 'UNetResNet34'#'RESNET34', 'RESNET18', 'INCEPTION_V3', 'BNINCEPTION', 'SEResnet50'\n",
    "#AUX_LOGITS = True#False, only for 'INCEPTION_V3'\n",
    "print('====MODEL ACHITECTURE: %s===='%MODEL)\n",
    "\n",
    "device = set_n_get_device(\"0\", data_device_id=\"cuda:0\")#0, 1, 2, 3, IMPORTANT: data_device_id is set to free gpu for storing the model, e.g.\"cuda:1\"\n",
    "multi_gpu = None#[0, 1]#use 2 gpus\n",
    "\n",
    "SEED = 3456#5678#4567#3456#2345#1234\n",
    "debug = False# if True, load 100 samples\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of trainset (for training):  9607\n",
      "Count of validset (for training):  1068\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = prepare_trainset(BATCH_SIZE, NUM_WORKERS, SEED, IMG_SIZE, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1064, 1, 512, 512), (1064, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y should be makeup\n",
    "valid_mask, valid_clf = [], []\n",
    "for i, (image, masks) in enumerate(val_dl):\n",
    "    truth_mask = masks#.to(device=device, dtype=torch.float)\n",
    "    truth_clf = (truth_mask.sum(dim=2).sum(dim=2)!=0).cpu().float().numpy()\n",
    "    valid_mask.append(truth_mask)\n",
    "    valid_clf.append(truth_clf)\n",
    "\n",
    "valid_mask = np.concatenate(valid_mask, axis=0)\n",
    "valid_clf = np.concatenate(valid_clf, axis=0)\n",
    "valid_mask.shape, valid_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNetResNet34(debug=False).cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoint/multitask_UNetResNet34_512_v2_seed3456/best.pth.tar'\n",
    "net, _ = load_checkpoint(checkpoint_path, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/data/endi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/bigdata/data/endi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 46s, sys: 2min 57s, total: 20min 43s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# predict_proba\n",
    "net.set_mode('valid')#.module\n",
    "preds_mask, preds_clf = predict_proba(net, val_dl, device, multi_gpu=False, mode='valid', tta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1064, 512, 512), (1064, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_mask.shape, preds_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## search for best thresholds\n",
    "def calculate_dice(logit_mask, logit_clf, truth_mask, \n",
    "                   CLF_NONEMPTY_THRESHOLD=0.75, MASK_THRESHOLD=0.22, EMPTY_THRESHOLD=400):\n",
    "    IMG_SIZE = logit_mask.shape[-1] #512\n",
    "    logit_mask = sigmoid(logit_mask)#.reshape(n, -1)\n",
    "    pred_mask = (logit_mask>MASK_THRESHOLD).astype(np.int)\n",
    "    logit_clf = sigmoid(logit_clf)\n",
    "    #option 1\n",
    "    pred_clf = (logit_clf>CLF_NONEMPTY_THRESHOLD).astype(np.int)\n",
    "    pred_mask[pred_clf.reshape(-1,)==0, ] = 0\n",
    "    #option 2\n",
    "    #pred_clf = (pred_mask.reshape(pred_mask.shape[0], -1).sum(axis=1)<EMPTY_THRESHOLD).astype(np.int)\n",
    "    #pred_mask[pred_clf.reshape(-1,)==1, ] = 0\n",
    "    return dice_overall(pred_mask, truth_mask)\n",
    "\n",
    "def dice_overall(pred_mask, truth_mask, eps=1e-8):\n",
    "    n = pred_mask.shape[0]\n",
    "    pred_mask = pred_mask.reshape(n, -1)\n",
    "    truth_mask = truth_mask.reshape(n, -1)\n",
    "    intersect = (pred_mask * truth_mask).sum(axis=1).astype(np.float)\n",
    "    union = (pred_mask + truth_mask).sum(axis=1).astype(np.float)\n",
    "    return ((2.0*intersect + eps) / (union+eps)).mean()\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85263350a660454d9e0c846b2020c98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLF_NONEMPTY_THRESHOLD: 0.690000, MASK_THRESHOLD: 0.300000, dice_score: 0.828422\n",
      "CLF_NONEMPTY_THRESHOLD: 0.690000, MASK_THRESHOLD: 0.320000, dice_score: 0.828889\n",
      "CLF_NONEMPTY_THRESHOLD: 0.690000, MASK_THRESHOLD: 0.340000, dice_score: 0.829131\n",
      "CLF_NONEMPTY_THRESHOLD: 0.690000, MASK_THRESHOLD: 0.360000, dice_score: 0.829294\n",
      "CLF_NONEMPTY_THRESHOLD: 0.690000, MASK_THRESHOLD: 0.380000, dice_score: 0.829329\n",
      "CLF_NONEMPTY_THRESHOLD: 0.690000, MASK_THRESHOLD: 0.400000, dice_score: 0.829202\n",
      "CLF_NONEMPTY_THRESHOLD: 0.700000, MASK_THRESHOLD: 0.300000, dice_score: 0.830302\n",
      "CLF_NONEMPTY_THRESHOLD: 0.700000, MASK_THRESHOLD: 0.320000, dice_score: 0.830769\n",
      "CLF_NONEMPTY_THRESHOLD: 0.700000, MASK_THRESHOLD: 0.340000, dice_score: 0.831011\n",
      "CLF_NONEMPTY_THRESHOLD: 0.700000, MASK_THRESHOLD: 0.360000, dice_score: 0.831174\n",
      "CLF_NONEMPTY_THRESHOLD: 0.700000, MASK_THRESHOLD: 0.380000, dice_score: 0.831208\n",
      "CLF_NONEMPTY_THRESHOLD: 0.700000, MASK_THRESHOLD: 0.400000, dice_score: 0.831081\n",
      "CLF_NONEMPTY_THRESHOLD: 0.710000, MASK_THRESHOLD: 0.300000, dice_score: 0.830264\n",
      "CLF_NONEMPTY_THRESHOLD: 0.710000, MASK_THRESHOLD: 0.320000, dice_score: 0.830725\n",
      "CLF_NONEMPTY_THRESHOLD: 0.710000, MASK_THRESHOLD: 0.340000, dice_score: 0.830973\n",
      "CLF_NONEMPTY_THRESHOLD: 0.710000, MASK_THRESHOLD: 0.360000, dice_score: 0.831151\n",
      "CLF_NONEMPTY_THRESHOLD: 0.710000, MASK_THRESHOLD: 0.380000, dice_score: 0.831187\n",
      "CLF_NONEMPTY_THRESHOLD: 0.710000, MASK_THRESHOLD: 0.400000, dice_score: 0.831082\n"
     ]
    }
   ],
   "source": [
    "CLF_NONEMPTY_THRESHOLD_candidate = np.arange(0.69, 0.71, 0.01)\n",
    "#EMPTY_THRESHOLD_candidate = np.arange(1440, 1540, 20)#np.arange(350, 450, 10) for IMG_SIZE=256\n",
    "MASK_THRESHOLD_candidate = np.arange(0.3, 0.4, 0.02)\n",
    "#M, N = len(EMPTY_THRESHOLD_candidate), len(MASK_THRESHOLD_candidate)\n",
    "M, N = len(CLF_NONEMPTY_THRESHOLD_candidate), len(MASK_THRESHOLD_candidate)\n",
    "best_threshold = None\n",
    "best_score = 0\n",
    "\n",
    "for i in tqdm_notebook(range(M)):\n",
    "    CLF_NONEMPTY_THRESHOLD = CLF_NONEMPTY_THRESHOLD_candidate[i]\n",
    "    #EMPTY_THRESHOLD = EMPTY_THRESHOLD_candidate[i]\n",
    "    for j in range(N):\n",
    "        MASK_THRESHOLD = MASK_THRESHOLD_candidate[j]\n",
    "        dice_score = calculate_dice(preds_mask, preds_clf, valid_mask.squeeze(1), \n",
    "                                    CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD, None)\n",
    "        print('CLF_NONEMPTY_THRESHOLD: %f, MASK_THRESHOLD: %f, dice_score: %f'%(CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD, dice_score))\n",
    "        #print('EMPTY_THRESHOLD: %f, MASK_THRESHOLD: %f, dice_score: %f'%(EMPTY_THRESHOLD, MASK_THRESHOLD, dice_score))\n",
    "        if dice_score>best_score:\n",
    "            #best_threshold = [EMPTY_THRESHOLD, MASK_THRESHOLD]\n",
    "            best_threshold = [CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD]\n",
    "            best_score = dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.38000000000000006, 0.8312083418598146)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EMPTY_THRESHOLD, MASK_THRESHOLD = best_threshold\n",
    "# EMPTY_THRESHOLD, MASK_THRESHOLD, best_score\n",
    "CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD = best_threshold\n",
    "CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(logit_mask, logit_clf, CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD, EMPTY_THRESHOLD):\n",
    "    \"\"\"Transform each prediction into mask.\n",
    "    input shape: (256, 256)\n",
    "    \"\"\"\n",
    "    #pred mask 0-1 pixel-wise\n",
    "    #n = logit.shape[0]\n",
    "    IMG_SIZE = logit_mask.shape[-1] #256\n",
    "    #logit = torch.sigmoid(torch.from_numpy(logit)).view(n, -1)\n",
    "    #pred = (logit>MASK_THRESHOLD).long()\n",
    "    #pred[pred.sum(dim=1) < EMPTY_THRESHOLD, ] = 0 #bug here, found it, the bug is input shape is (256, 256) not (16,256,256)\n",
    "    \n",
    "    logit_mask = sigmoid(logit_mask)#.reshape(n, -1)\n",
    "    pred_mask = (logit_mask>MASK_THRESHOLD).astype(np.int)\n",
    "    logit_clf = sigmoid(logit_clf)\n",
    "    pred_clf = (logit_clf>CLF_NONEMPTY_THRESHOLD).astype(np.int)\n",
    "\n",
    "    if pred_clf==0:\n",
    "        return np.zeros(pred_mask.shape).astype(np.int)\n",
    "    else:\n",
    "        return pred_mask\n",
    "\n",
    "#     logit_mask = sigmoid(logit_mask)#.reshape(n, -1)\n",
    "#     pred_mask = (logit_mask>MASK_THRESHOLD).astype(np.int)\n",
    "    \n",
    "#     if pred_mask.sum() < EMPTY_THRESHOLD:\n",
    "#         return np.zeros(pred_mask.shape).astype(np.int)\n",
    "#     else:\n",
    "#         return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualize predicted masks\n",
    "rows = 10\n",
    "\n",
    "cnt = 0\n",
    "for idx, (img, mask) in enumerate(val_dl):\n",
    "    for j in range(BATCH_SIZE):\n",
    "        not_empty = mask[j][0].sum()>0\n",
    "        if not_empty:\n",
    "            cnt+=1\n",
    "            pred_mask = predict_mask(preds_mask[idx*BATCH_SIZE+j], preds_clf[idx*BATCH_SIZE+j], CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD, None)\n",
    "#             pred_clf = sigmoid()\n",
    "#             if pred_clf>0.85:\n",
    "#                 pred_empty = True\n",
    "#             else:\n",
    "#                 pred_empty = False\n",
    "            #if pred_mask.sum()==0:\n",
    "            #    continue\n",
    "            fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "            ax0.imshow(img[j][0].numpy(), plt.cm.bone)\n",
    "            ax1.imshow(mask[j][0], vmin=0, vmax=1, cmap=\"Reds\")\n",
    "            ax2.imshow(pred_mask, vmin=0, vmax=1, cmap=\"Blues\")\n",
    "            if not_empty.item():\n",
    "                ax1.set_title('Targets(Has Mask)')\n",
    "            else:\n",
    "                ax1.set_title('Targets(Empty)')\n",
    "            if pred_mask.sum()==0:#pred_empty:\n",
    "                ax2.set_title('Predict Empty')\n",
    "            else:\n",
    "                ax2.set_title('Predict Mask')\n",
    "            ax0.set_title('idx=%d'%(idx*16+j))\n",
    "        if cnt>rows:\n",
    "            break\n",
    "    if cnt>rows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from metrics import dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8263)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice(torch.from_numpy(np.expand_dims(preds_valid, 1)), torch.from_numpy(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.shape, preds_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8021)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice(torch.zeros(y_valid.shape), torch.from_numpy(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from dataset_unet import prepare_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1377, '1.2.276.0.7230010.3.1.4.8323329.6160.1517875196.806852')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fnames = [f.split('/')[-1][:-4] for f in glob.glob('data/processed/test/*')]\n",
    "len(test_fnames), test_fnames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = prepare_testset(BATCH_SIZE, NUM_WORKERS, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/data/endi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/bigdata/data/endi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 3s, sys: 4min 18s, total: 28min 21s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds_mask, preds_clf = predict_proba(net, test_dl, device, multi_gpu=False, mode='test', tta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1377, 512, 512), (1377, 1))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_mask.shape, preds_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualize predicted masks\n",
    "total = 19\n",
    "\n",
    "fig=plt.figure(figsize=(15, 20))\n",
    "cnt = 0\n",
    "for idx, img in enumerate(test_dl):\n",
    "    for j in range(BATCH_SIZE):#BATCH_SIZE\n",
    "        cnt+=1\n",
    "        pred_mask = predict_mask(preds_mask[idx*BATCH_SIZE+j], preds_clf[idx*BATCH_SIZE+j], \n",
    "                                 CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD, None)\n",
    "        #if pred_mask.float().mean()==0:\n",
    "        #    continue\n",
    "        ax = fig.add_subplot(5, 4, cnt)\n",
    "        plt.imshow(img[j][0].numpy(), plt.cm.bone)\n",
    "        plt.imshow(pred_mask, alpha=0.3, cmap=\"Reds\")\n",
    "        if pred_mask.sum()>0:\n",
    "            plt.title('Predict Mask')\n",
    "        else:\n",
    "            plt.title('Predict Empty')\n",
    "        if cnt>total:\n",
    "            break\n",
    "    if cnt>total:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from mask_functions import mask2rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1377, 512, 512), (1377, 1))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_mask.shape, preds_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa5a74594674b3d9f36a9cb3b0c23b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1377), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6160.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.582.1517875163...</td>\n",
       "      <td>622766 2 1022 2 1018 10 2 8 1004 10 2 8 1002 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6985.151787520...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.5865.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6187.151787519...</td>\n",
       "      <td>248092 6 6 2 1010 6 6 2 1006 8 1016 8 1012 12 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ImageId  \\\n",
       "0  1.2.276.0.7230010.3.1.4.8323329.6160.151787519...   \n",
       "1  1.2.276.0.7230010.3.1.4.8323329.582.1517875163...   \n",
       "2  1.2.276.0.7230010.3.1.4.8323329.6985.151787520...   \n",
       "3  1.2.276.0.7230010.3.1.4.8323329.5865.151787519...   \n",
       "4  1.2.276.0.7230010.3.1.4.8323329.6187.151787519...   \n",
       "\n",
       "                                       EncodedPixels  \n",
       "0                                                 -1  \n",
       "1  622766 2 1022 2 1018 10 2 8 1004 10 2 8 1002 2...  \n",
       "2                                                 -1  \n",
       "3                                                 -1  \n",
       "4  248092 6 6 2 1010 6 6 2 1006 8 1016 8 1012 12 ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Step 1: Generate rle encodings (images are first converted to the original size)\n",
    "rles = []\n",
    "for idx in tqdm_notebook(range(preds_mask.shape[0])):#p is logit from model\n",
    "    pred_mask, pred_clf = preds_mask[idx], preds_clf[idx]\n",
    "    pred_mask = predict_mask(pred_mask, pred_clf, CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD, None)\n",
    "    if pred_mask.sum()>0:#predicted non-empty mask\n",
    "        im = PIL.Image.fromarray((pred_mask.T*255).astype(np.uint8)).resize((1024,1024))\n",
    "        im = np.asarray(im)\n",
    "        rles.append(mask2rle(im, 1024, 1024))\n",
    "    else: rles.append('-1')\n",
    "    \n",
    "sub_df = pd.DataFrame({'ImageId': test_fnames, 'EncodedPixels': rles})\n",
    "print(len(sub_df.index))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the correctness of transformation before submit\n",
    "i = 22\n",
    "pred_mask = predict_mask(preds_mask[i], preds_clf[i], CLF_NONEMPTY_THRESHOLD, MASK_THRESHOLD, None)\n",
    "im = PIL.Image.fromarray((pred_mask.T*255).astype(np.uint8)).resize((1024,1024))\n",
    "im = np.asarray(im)\n",
    "im.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbf61d79748>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission/0722_multitask_unet_512_v2_seed3456.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2: the sub is Instance Segmentation, so need to split mask into instances of masks\n",
    "def split_mask(mask):\n",
    "    MASK_THRESHOLD = 0.22\n",
    "    EMPTY_THRESHOLD = 30 #ignore predictions composed of 30 pixels or less\n",
    "    #split disconnected masks with ndimage.label function\n",
    "    labled,n_objs = ndimage.label(mask > MASK_THRESHOLD)\n",
    "    result = []\n",
    "    n_pixels = []\n",
    "    for i in range(n_objs):\n",
    "        obj = (labled == i + 1).astype(int)\n",
    "        if obj.sum() > EMPTY_THRESHOLD:\n",
    "            result.append(obj)\n",
    "            n_pixels.append(obj.sum())\n",
    "    #sort masks based on the number of pixels\n",
    "    result = [x for _,x in sorted(zip(n_pixels,result),reverse=True,key=lambda x:x[0])]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abb2b78c08f4e829008ba69e32418b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1377), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3min 11s, sys: 3.99 s, total: 3min 15s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rles = []\n",
    "ids_c = []\n",
    "for p,idx in tqdm_notebook(zip(preds_test, test_fnames), total=len(preds_test)):\n",
    "    pred_mask = predict_mask(p)\n",
    "    if pred_mask.sum() > 0:\n",
    "        masks = split_mask(pred_mask)#to_np\n",
    "        for mask in masks:\n",
    "            ids_c.append(idx)\n",
    "            im = PIL.Image.fromarray((mask.T*255).astype(np.uint8)).resize((1024,1024))\n",
    "            im = np.asarray(im)\n",
    "            rles.append(mask2rle(im, 1024, 1024))\n",
    "        if len(masks) == 0:\n",
    "            rles.append('-1')\n",
    "            ids_c.append(idx)\n",
    "    else: \n",
    "        rles.append('-1')\n",
    "        ids_c.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6160.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.582.1517875163...</td>\n",
       "      <td>602280 20 1004 20 1004 20 1004 20 996 56 968 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.582.1517875163...</td>\n",
       "      <td>844468 8 1016 8 1016 8 1016 8 1016 8 1016 8 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6985.151787520...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.5865.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ImageId  \\\n",
       "0  1.2.276.0.7230010.3.1.4.8323329.6160.151787519...   \n",
       "1  1.2.276.0.7230010.3.1.4.8323329.582.1517875163...   \n",
       "2  1.2.276.0.7230010.3.1.4.8323329.582.1517875163...   \n",
       "3  1.2.276.0.7230010.3.1.4.8323329.6985.151787520...   \n",
       "4  1.2.276.0.7230010.3.1.4.8323329.5865.151787519...   \n",
       "\n",
       "                                       EncodedPixels  \n",
       "0                                                 -1  \n",
       "1  602280 20 1004 20 1004 20 1004 20 996 56 968 5...  \n",
       "2  844468 8 1016 8 1016 8 1016 8 1016 8 1016 8 10...  \n",
       "3                                                 -1  \n",
       "4                                                 -1  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.DataFrame({'ImageId': ids_c, 'EncodedPixels': rles})\n",
    "print(len(sub_df))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission/0708_unet_seed1234_split.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
